{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSVSBh0xBDIC",
        "outputId": "55ac8f3d-f421-496a-b710-31b078988bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mir_eval\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.11.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mir_eval) (0.18.3)\n",
            "Collecting packaging~=23.1 (from mido>=1.1.16->pretty_midi)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi, mir_eval\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592289 sha256=2f11e825cc738f75eb5266b694719227c4bd95b37ed5281b657d51ca1eb87a91\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "  Building wheel for mir_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir_eval: filename=mir_eval-0.7-py3-none-any.whl size=100704 sha256=9c2a68b7a3a7c58cc27d959775d7317d4406685ea7df52b6aedf653237813b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/2f/0d/dda9c4c77a170e21356b6afa2f7d9bb078338634ba05d94e3f\n",
            "Successfully built pretty_midi mir_eval\n",
            "Installing collected packages: packaging, mir_eval, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed mido-1.3.2 mir_eval-0.7 packaging-23.2 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pretty_midi mir_eval numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK9FpIshBIWY",
        "outputId": "6f98885e-a1b5-4762-eabe-8b746a89351c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove _basic_pitch from midi files\n",
        "# import os\n",
        "\n",
        "# def rename_files(folder_path):\n",
        "#     for filename in os.listdir(folder_path):\n",
        "#         if '_basic_pitch' in filename:\n",
        "#             new_filename = filename.replace('_basic_pitch', '')\n",
        "#             old_file_path = os.path.join(folder_path, filename)\n",
        "#             new_file_path = os.path.join(folder_path, new_filename)\n",
        "\n",
        "#             os.rename(old_file_path, new_file_path)\n",
        "#             print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
        "\n",
        "# os.chdir(\"/content/drive/MyDrive/Dissertation Code\")\n",
        "# folder_path = \"OMAPS/evaluation/basic-pitch\"\n",
        "# rename_files(folder_path)\n"
      ],
      "metadata": {
        "id": "AuKkCEQ-Dsrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pretty_midi\n",
        "import mir_eval\n",
        "import numpy as np\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Dissertation Code\")\n",
        "\n",
        "# def midi_to_freq(midi):\n",
        "#     if midi >= 0:\n",
        "#         return 2 ** ((midi - 69) / 12) * 440\n",
        "#     else:\n",
        "#         return 0\n",
        "\n",
        "# def read_note_data_from_text(file_path):\n",
        "#     intervals = []\n",
        "#     pitches = []\n",
        "#     with open(file_path, 'r') as file:\n",
        "#         for line in file:\n",
        "#             start, end, pitch = line.strip().split()\n",
        "#             intervals.append([float(start), float(end)])\n",
        "#             pitches.append(int(pitch))\n",
        "#     # print(np.array(intervals))\n",
        "#     return np.array(intervals), np.array(pitches)\n",
        "\n",
        "def read_note_data_from_text(file_path): # For OMAPS2\n",
        "    \"\"\"Read note data from a text file.\"\"\"\n",
        "    intervals = []\n",
        "    pitches = []\n",
        "    velocities = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            columns = line.strip().split()\n",
        "            # Extract onset, offset, pitch and velocity\n",
        "            start, end, pitch, velocity = columns\n",
        "            intervals.append([float(start), float(end)])\n",
        "            pitches.append(int(pitch))\n",
        "            velocities.append(int(velocity))\n",
        "    return np.array(intervals), np.array(pitches)\n",
        "\n",
        "# def prepare_data_for_evaluation(midi_file, cc_list=[64, 67], split_pedal=False):\n",
        "#     midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "#     notes = []\n",
        "#     for instrument in midi_data.instruments:\n",
        "#         # Filter by piano instruments\n",
        "#         if instrument.program in range(0, 8):\n",
        "#             notes.extend(instrument.notes)\n",
        "\n",
        "#     # Process notes and pedals\n",
        "#     intervals = []\n",
        "#     pitches = []\n",
        "#     velocities = []\n",
        "#     for note in notes:\n",
        "#         if note.pitch >= 0 or -note.pitch in cc_list:\n",
        "#             intervals.append([note.start, note.end])\n",
        "#             pitches.append(midi_to_freq(note.pitch))\n",
        "#             velocities.append(note.velocity)\n",
        "\n",
        "#     intervals = np.array(intervals)\n",
        "#     pitches = np.array(pitches)\n",
        "#     velocities = np.array(velocities)\n",
        "\n",
        "#     return intervals, pitches, velocities\n",
        "\n",
        "# def evaluate_midi(predicted_midi_file, truth_midi_file):\n",
        "#     predicted_intervals, predicted_pitches, _ = prepare_data_for_evaluation(predicted_midi_file)\n",
        "#     truth_intervals, truth_pitches, _ = prepare_data_for_evaluation(truth_midi_file)\n",
        "\n",
        "#     precision, recall, f1_score, _ = mir_eval.transcription.precision_recall_f1_overlap(\n",
        "#         truth_intervals, truth_pitches, predicted_intervals, predicted_pitches)\n",
        "\n",
        "#     return precision, recall, f1_score\n",
        "\n",
        "def prepare_data_for_evaluation(midi_file):\n",
        "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "    intervals = []\n",
        "    pitches = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        # For non-drum instruments\n",
        "        for note in instrument.notes:\n",
        "            intervals.append([note.start, note.end])\n",
        "            pitches.append(note.pitch)\n",
        "    # print(np.array(intervals))\n",
        "    return np.array(intervals), np.array(pitches)\n",
        "\n",
        "def read_note_data_from_npy(file_path):\n",
        "    # Load data from the .npy file\n",
        "    data = np.load(file_path)\n",
        "\n",
        "    # Find onset, offset and pitch\n",
        "    intervals = data[:, :2]\n",
        "    pitches = data[:, 2]\n",
        "\n",
        "    return intervals, pitches\n",
        "\n",
        "def evaluate_midi(predicted_midi_file, truth_midi_file):\n",
        "    predicted_intervals, predicted_pitches = prepare_data_for_evaluation(predicted_midi_file)\n",
        "    truth_intervals, truth_pitches = read_note_data_from_text(truth_midi_file)\n",
        "\n",
        "    # print(\"done\")\n",
        "\n",
        "    # mir_eval.transcription.validate(truth_intervals, truth_pitches, predicted_intervals, predicted_pitches)\n",
        "\n",
        "    # Full evaluation considering only onset\n",
        "    full_precision, full_recall, full_f1_score, _ = mir_eval.transcription.precision_recall_f1_overlap(\n",
        "        truth_intervals, truth_pitches, predicted_intervals, predicted_pitches,\n",
        "        onset_tolerance=0.05, pitch_tolerance=50.0, offset_ratio=None, offset_min_tolerance=0.05, strict=False, beta=1.0)\n",
        "\n",
        "    # Onset evaluation\n",
        "    onset_precision, onset_recall, onset_f1_score = mir_eval.transcription.onset_precision_recall_f1(\n",
        "        truth_intervals, predicted_intervals, onset_tolerance=0.05, strict=False, beta=1.0)\n",
        "\n",
        "    # Offset evaluation\n",
        "    offset_precision, offset_recall, offset_f1_score = mir_eval.transcription.offset_precision_recall_f1(\n",
        "        truth_intervals, predicted_intervals, offset_ratio=0.2, offset_min_tolerance=0.05, strict=False, beta=1.0)\n",
        "\n",
        "    absolute_error = mir_eval.alignment.absolute_error(truth_intervals, predicted_intervals)\n",
        "\n",
        "    return {\n",
        "        'full': (full_precision, full_recall, full_f1_score),\n",
        "        'onset': (onset_precision, onset_recall, onset_f1_score),\n",
        "        'offset': (offset_precision, offset_recall, offset_f1_score).\n",
        "        'absoulte_error': absolute_error\n",
        "    }\n",
        "\n",
        "# def evaluate_folders(predicted_folder, truth_folder, output_file):\n",
        "#     precisions = []\n",
        "#     recalls = []\n",
        "#     f1_scores = []\n",
        "\n",
        "#     with open(output_file, 'w') as f:\n",
        "#         for predicted_file in os.listdir(predicted_folder):\n",
        "#             if not predicted_file.endswith('.mid'):\n",
        "#                 continue  # Skip files that are not MIDI files\n",
        "#             predicted_path = os.path.join(predicted_folder, predicted_file)\n",
        "#             # Change the extension\n",
        "#             truth_file = predicted_file.rsplit('.', 1)[0] + '.txt'\n",
        "#             truth_path = os.path.join(truth_folder, truth_file)\n",
        "\n",
        "#             if os.path.isfile(truth_path):\n",
        "#                 try:\n",
        "#                     precision, recall, f1_score = evaluate_midi(predicted_path, truth_path)\n",
        "#                     precisions.append(precision)\n",
        "#                     recalls.append(recall)\n",
        "#                     f1_scores.append(f1_score)\n",
        "\n",
        "#                     f.write(f\"{predicted_file}: Precision={precision}, Recall={recall}, F1 Score={f1_score}\\n\")\n",
        "#                 except ValueError as e:\n",
        "#                     print(f\"Error processing {predicted_file}: {e}\")\n",
        "#                     f.write(f\"Error processing {predicted_file}: {e}\\n\")\n",
        "#             else:\n",
        "#                 print(f\"Ground truth file for {predicted_file} not found\")\n",
        "#                 f.write(f\"Ground truth file for {predicted_file} not found\\n\")\n",
        "\n",
        "#         if precisions:\n",
        "#             combined_precision = np.mean(precisions)\n",
        "#             combined_recall = np.mean(recalls)\n",
        "#             combined_f1_score = np.mean(f1_scores)\n",
        "\n",
        "#             # Write the combined metrics to the file\n",
        "#             f.write(f\"\\nCombined Precision: {combined_precision}\\n\")\n",
        "#             f.write(f\"Combined Recall: {combined_recall}\\n\")\n",
        "#             f.write(f\"Combined F1 Score: {combined_f1_score}\\n\")\n",
        "#         else:\n",
        "#             combined_precision = combined_recall = combined_f1_score = 0\n",
        "#             f.write(\"No MIDI files were successfully processed\\n\")\n",
        "\n",
        "#     return combined_precision, combined_recall, combined_f1_score\n",
        "\n",
        "# # Usage example:\n",
        "# os.chdir(\"/content/drive/MyDrive/Dissertation Code\")\n",
        "\n",
        "# output_file = \"test_evaluation_results.txt\"\n",
        "# predicted_folder = \"OMAPS/evaluation/skipping-the-frame-level\"\n",
        "# truth_folder = \"OMAPS/complete/text\"\n",
        "\n",
        "# combined_precision, combined_recall, combined_f1_score = evaluate_folders(predicted_folder, truth_folder, output_file)\n",
        "\n",
        "# print(f\"Combined Precision: {combined_precision}\")\n",
        "# print(f\"Combined Recall: {combined_recall}\")\n",
        "# print(f\"Combined F1 Score: {combined_f1_score}\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_folders(predicted_folder, truth_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    full_eval_path = os.path.join(output_folder, \"full_evaluation_results.txt\")\n",
        "    onset_eval_path = os.path.join(output_folder, \"onset_evaluation_results.txt\")\n",
        "    offset_eval_path = os.path.join(output_folder, \"offset_evaluation_results.txt\")\n",
        "    full_avg_path = os.path.join(output_folder, \"full_evaluation_average.txt\")\n",
        "    onset_avg_path = os.path.join(output_folder, \"onset_evaluation_average.txt\")\n",
        "    offset_avg_path = os.path.join(output_folder, \"offset_evaluation_average.txt\")\n",
        "\n",
        "    full_scores = []\n",
        "    onset_scores = []\n",
        "    offset_scores = []\n",
        "\n",
        "    with open(full_eval_path, 'w') as full_file, open(onset_eval_path, 'w') as onset_file, open(offset_eval_path, 'w') as offset_file:\n",
        "        full_file.write(\"File, Precision, Recall, F1 Score\\n\")\n",
        "        onset_file.write(\"File, Precision, Recall, F1 Score\\n\")\n",
        "        offset_file.write(\"File, Precision, Recall, F1 Score\\n\")\n",
        "\n",
        "    for predicted_file in os.listdir(predicted_folder):\n",
        "        if not predicted_file.endswith('.mid'):\n",
        "            continue\n",
        "\n",
        "        predicted_path = os.path.join(predicted_folder, predicted_file)\n",
        "        truth_file = predicted_file.rsplit('.', 1)[0] + '.txt'\n",
        "        truth_path = os.path.join(truth_folder, truth_file)\n",
        "\n",
        "        if os.path.isfile(truth_path):\n",
        "            try:\n",
        "                results = evaluate_midi(predicted_path, truth_path)\n",
        "\n",
        "                full_scores.append(results['full'])\n",
        "                onset_scores.append(results['onset'])\n",
        "                offset_scores.append(results['offset'])\n",
        "\n",
        "                with open(full_eval_path, 'a') as full_file, open(onset_eval_path, 'a') as onset_file, open(offset_eval_path, 'a') as offset_file:\n",
        "                    full_file.write(f\"{predicted_file}, {results['full'][0]}, {results['full'][1]}, {results['full'][2]}\\n\")\n",
        "                    onset_file.write(f\"{predicted_file}, {results['onset'][0]}, {results['onset'][1]}, {results['onset'][2]}\\n\")\n",
        "                    offset_file.write(f\"{predicted_file}, {results['offset'][0]}, {results['offset'][1]}, {results['offset'][2]}\\n\")\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(\"Error\")\n",
        "                pass\n",
        "\n",
        "    def calculate_and_save_averages(scores, file_path):\n",
        "        if scores:\n",
        "            avg_precision = sum(score[0] for score in scores) / len(scores)\n",
        "            avg_recall = sum(score[1] for score in scores) / len(scores)\n",
        "            avg_f1 = sum(score[2] for score in scores) / len(scores)\n",
        "            with open(file_path, 'w') as file:\n",
        "                file.write(f\"Average Precision, Average Recall, Average F1 Score\\n\")\n",
        "                file.write(f\"{avg_precision}, {avg_recall}, {avg_f1}\\n\")\n",
        "\n",
        "    calculate_and_save_averages(full_scores, full_avg_path)\n",
        "    calculate_and_save_averages(onset_scores, onset_avg_path)\n",
        "    calculate_and_save_averages(offset_scores, offset_avg_path)\n",
        "\n",
        "predicted_folder = \"OMAPS2/evaluation/skipping-the-frame-level\"\n",
        "truth_folder = \"OMAPS2/complete/text\"\n",
        "\n",
        "evaluate_folders(predicted_folder, truth_folder, predicted_folder)\n"
      ],
      "metadata": {
        "id": "_ybuVIrwBKYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f100290-e8f0-4011-dba1-17e0de447ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n",
            "Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "3NpmUcLrMEsA",
        "outputId": "fc7c9843-7973-4636-d12e-10f59d9cf124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "absolute_error() missing 2 required positional arguments: 'reference_timestamps' and 'estimated_timestamps'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-355a1db11cc0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmir_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: absolute_error() missing 2 required positional arguments: 'reference_timestamps' and 'estimated_timestamps'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_intervals, predicted_pitches = read_note_data_from_npy(\"OMAPS2/evaluation/attack-delay/01_01-transcription.npy\")\n",
        "truth_intervals, truth_pitches = read_note_data_from_text(\"OMAPS2/complete/text/01_01.txt\")\n",
        "\n",
        "print(\"Predicted Intervals:\", predicted_intervals)\n",
        "print(\"Predicted Pitches:\", predicted_pitches)\n",
        "print(\"Truth Intervals:\", truth_intervals)\n",
        "print(\"Truth Pitches:\", truth_pitches)"
      ],
      "metadata": {
        "id": "OqpXCPLgNk4g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5722da34-afad-49ec-8d3b-03b7d57ed891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'OMAPS2/evaluation/attack-delay/01_01-transcription.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-642a3be4d7a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_intervals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_pitches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_note_data_from_npy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OMAPS2/evaluation/attack-delay/01_01-transcription.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtruth_intervals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_pitches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_note_data_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OMAPS2/complete/text/01_01.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Intervals:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_intervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Pitches:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_pitches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-f6c9267b9293>\u001b[0m in \u001b[0;36mread_note_data_from_npy\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m\"\"\"Read note data from a .npy file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Load data from the .npy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Extract start time, end time, and pitch from the loaded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OMAPS2/evaluation/attack-delay/01_01-transcription.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzHXA8d5SPOj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}